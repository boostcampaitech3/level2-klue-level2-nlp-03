{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화한 뒤 sequence length 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 04:18:04.531798: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "#  library\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    ElectraModel,\n",
    "    ElectraTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    subject_entity = []\n",
    "    object_entity = []\n",
    "    for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        i = i[1:-1].split(',')[0].split(':')[1]\n",
    "        j = j[1:-1].split(',')[0].split(':')[1]\n",
    " \n",
    "        subject_entity.append(i)\n",
    "        object_entity.append(j)\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
    "    return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset = load_data('../dataset/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        list(dataset['sentence']),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, \n",
    "        truncation=False, # 나중에 True로 변경해야 함\n",
    "        #max_length=256, \n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_dataset(out_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    11, 29830,  ...,     0,     0,     0],\n",
       "        [    2,    11,  3772,  ...,     0,     0,     0],\n",
       "        [    2,    11,  4104,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    11, 18272,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15710,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15437,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32470, 241])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    11, 29830,  ...,     0,     0,     0],\n",
       "        [    2,    11,  3772,  ...,     0,     0,     0],\n",
       "        [    2,    11,  4104,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    11, 18272,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15710,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15437,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(list(out_dataset['sentence'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_roberta = tokenized_dataset(out_dataset, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32470, 241])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_roberta.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    11, 29830,  ...,     1,     1,     1],\n",
       "        [    0,    11,  3772,  ...,     1,     1,     1],\n",
       "        [    0,    11,  4104,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,    11, 18272,  ...,     1,     1,     1],\n",
       "        [    0,    11, 15710,  ...,     1,     1,     1],\n",
       "        [    0,    11, 15437,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_roberta.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer.tokenize(list(out_dataset['sentence'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
