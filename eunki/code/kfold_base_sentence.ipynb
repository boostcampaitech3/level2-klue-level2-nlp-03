{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 기반한 데이터셋 만들기\n",
    "\n",
    "앞서, 게시판에 재학님이 올리신 것을 바탕으로 모두가 동일한 문장을 제거하면 될 것 같습니다.\n",
    "\n",
    "저는 그 이후 문장이 중복되는 경우 하나의 데이터셋으로만 흘러가도록 만드는 것을 구현하고자 했습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 loda_data.py와 train에 있는 label_to_num을 활용해 데이터를 불러오고     \n",
    "categorical labelling을 수행해주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>'비틀즈'</td>\n",
       "      <td>'조지 해리슨'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>'민주평화당'</td>\n",
       "      <td>'대안신당'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>'광주FC'</td>\n",
       "      <td>'한국프로축구연맹'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>'아성다이소'</td>\n",
       "      <td>'박정부'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>'요미우리 자이언츠'</td>\n",
       "      <td>'1967'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_entity  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          '비틀즈'   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...        '민주평화당'   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...         '광주FC'   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...        '아성다이소'   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...    '요미우리 자이언츠'   \n",
       "\n",
       "  object_entity                      label  \n",
       "0      '조지 해리슨'                no_relation  \n",
       "1        '대안신당'                no_relation  \n",
       "2    '한국프로축구연맹'              org:member_of  \n",
       "3         '박정부'  org:top_members/employees  \n",
       "4        '1967'                no_relation  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_data import *\n",
    "from train import label_to_num\n",
    "\n",
    "all_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "all_label = label_to_num(all_dataset['label'].values)\n",
    "\n",
    "all_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 레이블의 경우에는 list 형태로 되어 있었습니당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 20, 1, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복되는 문장 확인을 위한 is_duplicated column을 만들어주었습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>is_duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>'비틀즈'</td>\n",
       "      <td>'조지 해리슨'</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>'민주평화당'</td>\n",
       "      <td>'대안신당'</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>'광주FC'</td>\n",
       "      <td>'한국프로축구연맹'</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>'아성다이소'</td>\n",
       "      <td>'박정부'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>'요미우리 자이언츠'</td>\n",
       "      <td>'1967'</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_entity  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          '비틀즈'   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...        '민주평화당'   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...         '광주FC'   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...        '아성다이소'   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...    '요미우리 자이언츠'   \n",
       "\n",
       "  object_entity                      label  is_duplicated  \n",
       "0      '조지 해리슨'                no_relation          False  \n",
       "1        '대안신당'                no_relation          False  \n",
       "2    '한국프로축구연맹'              org:member_of          False  \n",
       "3         '박정부'  org:top_members/employees          False  \n",
       "4        '1967'                no_relation          False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset['is_duplicated'] = all_dataset['sentence'].duplicated(keep=False)\n",
    "all_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label의 경우에도 일단은 DataFrame으로 만들어주었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1      0\n",
       "2     20\n",
       "3      1\n",
       "4      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label = pd.DataFrame(train_label, columns=['label'])\n",
    "all_label.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 is_duplicated를 기반으로 데이터셋을 나누어주었습니다.    \n",
    "이를 바탕으로 코드 작성이 진행되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "not_duplicated_data = all_dataset[all_dataset['is_duplicated'] == False]\n",
    "not_duplicated_label = all_label.iloc[all_dataset[all_dataset['is_duplicated'] == False].index]\n",
    "\n",
    "duplicated_data = all_dataset[all_dataset['is_duplicated'] == True]\n",
    "duplicated_label = all_label.iloc[all_dataset[all_dataset['is_duplicated'] == True].index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초창기 버전\n",
    "\n",
    "처음 버전은 50% 확률로 train 혹은 valid 데이터셋이 선택되어 거기 있는 모든 중복 데이터를 다른 곳으로 옮기는 방식입니다.\n",
    "이 방식을 사용하면 한 번은 train에 모두 중복되지 않은 데이터, valid에는 중복된 데이터 + 중복되지 않은 데이터 일정량     \n",
    "두번째는 반대로 진행되게 됩니다.\n",
    "\n",
    "하지만 이런식으로 사용하면 train이나 valid에서 문제가 생길 것 같아 보류로 해두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20271 6494\n",
      "20307 6494\n",
      "20276 6494\n",
      "25976 5083\n",
      "20369 6494\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold= StratifiedKFold(n_splits= 5, shuffle= True, random_state= 42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(all_dataset, all_label)):\n",
    "    train_dataset= all_dataset.iloc[train_idx]\n",
    "    val_dataset= all_dataset.iloc[val_idx]\n",
    "    train_label= all_label.iloc[train_idx]\n",
    "    val_label= all_label.iloc[val_idx]\n",
    "    \n",
    "    train_dataset.reset_index(drop= True, inplace= True)\n",
    "    val_dataset.reset_index(drop= True, inplace= True)\n",
    "    \n",
    "    temp = []    \n",
    "    if random.random() > 0.5:\n",
    "        for val_idx in val_dataset.index:\n",
    "            if val_dataset['is_duplicated'].iloc[val_idx] == True:\n",
    "                train_dataset.append(val_dataset.iloc[val_idx])\n",
    "                temp.append(val_idx)\n",
    "        val_dataset.drop(temp, inplace= True, axis= 0)\n",
    "    else:\n",
    "        for train_idx in train_dataset.index:\n",
    "            if train_dataset['is_duplicated'].iloc[train_idx] == True:\n",
    "                val_dataset.append(train_dataset.iloc[train_idx])\n",
    "                temp.append(train_idx)\n",
    "        train_dataset.drop(temp, inplace= True, axis= 0)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분할은 그래도 나쁘지 않게 되는 모습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보완 버전\n",
    "\n",
    "해당 버전에서는 valid를 기반으로 진행됩니다    \n",
    "먼저, 문장들을 읽으며 is_duplicated인 경우에 중복되는 문장이 train에 있는지 확인합니     \n",
    "만약 train에 있는 경우 한 곳으로 몰아주기 위해 데이터를 train으로 옮겨줍니다.    \n",
    "그리고 해당 idx를 temp에 저장한 이후에 valid 데이터를 모두 처리한 후 drop을 시켜줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25976 5339\n",
      "25976 5314\n",
      "25976 5355\n",
      "25976 5337\n",
      "25976 5256\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "kfold = StratifiedKFold(n_splits= 5, shuffle= True, random_state= 42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(all_dataset, all_label)):\n",
    "    train_dataset= all_dataset.loc[train_idx]\n",
    "    val_dataset= all_dataset.loc[val_idx]\n",
    "    train_dataset.reset_index(drop= True, inplace= True)\n",
    "    val_dataset.reset_index(drop= True, inplace= True)\n",
    "    \n",
    "    temp = []    \n",
    "    \n",
    "    for val_idx in val_dataset.index:\n",
    "        if val_dataset['is_duplicated'].loc[val_idx] == True:\n",
    "            if val_dataset['sentence'].loc[val_idx] in train_dataset['sentence'].values:\n",
    "                train_dataset.append(val_dataset.loc[val_idx])\n",
    "                temp.append(val_idx)\n",
    "            \n",
    "    val_dataset.drop(temp, inplace= True, axis= 0)\n",
    "\n",
    "    print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분할은 위와 같이 깔끔하게 잘 됩니다.. 근데 왜 에러 날까요ㅠㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래는 아직 작업중인 코드들입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('/opt/ml/git/level2-klue-level2-nlp-03/eunki/code/dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "\n",
    "total_train_dataset = load_data(\"../dataset/train/train.csv\")\n",
    "total_train_label = label_to_num(total_train_dataset['label'].values)\n",
    "pd.DataFrame(data = total_train_label, columns=['label']).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_split(train_dataset, train_label, test_size=0.2, random_state=42, stratify=[train_dataset['label'], train_dataset['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, CSV_PATH, version):\n",
    "        self.version= version\n",
    "        self.data= self.load_data(CSV_PATH)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        \n",
    "        data= pd.read_csv(path)\n",
    "        \n",
    "        sub_entity, sub_type= [], []\n",
    "        obj_entity, obj_type= [], []\n",
    "        sub_idx, obj_idx= [], []\n",
    "        sentence= []\n",
    "\n",
    "        \"\"\"preprocess\"\"\"\n",
    "        for i, [x, y, z] in enumerate(zip(data['subject_entity'], data['object_entity'], data['sentence'])):\n",
    "            sub_typ= x[1:-1].split(':')[-1].split('\\'')[-2]\n",
    "            obj_typ= y[1:-1].split(':')[-1].split('\\'')[-2]\n",
    "            \n",
    "            for idx_i in range(len(x)):\n",
    "                if x[idx_i: idx_i+ 9]== 'start_idx':\n",
    "                    sub_start= int(x[idx_i+12:].split(',')[0].strip())\n",
    "                if x[idx_i: idx_i+7]== 'end_idx':\n",
    "                    sub_end= int(x[idx_i+10:].split(',')[0].strip())\n",
    "                \n",
    "                if y[idx_i: idx_i+ 9]== 'start_idx':\n",
    "                    obj_start= int(y[idx_i+12:].split(',')[0].strip())\n",
    "                if y[idx_i: idx_i+7]== 'end_idx':\n",
    "                    obj_end= int(y[idx_i+10:].split(',')[0].strip())\n",
    "            \n",
    "            sub_i= [sub_start, sub_end]\n",
    "            obj_i= [obj_start, obj_end]\n",
    "\n",
    "            sub_entity.append(z[sub_i[0]: sub_i[1]+1])\n",
    "            obj_entity.append(z[obj_i[0]: obj_i[1]+1])\n",
    "            sub_type.append(sub_typ); sub_idx.append(sub_i)\n",
    "            obj_type.append(obj_typ); obj_idx.append(obj_i)\n",
    "            \n",
    "            \"\"\"tokenize version\"\"\"\n",
    "            # if self.version== 'SUB':\n",
    "            #     if sub_i[0] < obj_i[0]:\n",
    "            #         z= z[:sub_i[0]] + '[SUB]'+ z[sub_i[0]: sub_i[1]+1] + '[/SUB]' + z[sub_i[1]+1:]\n",
    "            #         z= z[:obj_i[0]+11] + '[OBJ]'+ z[obj_i[0]+11: obj_i[1]+12]+ '[/OBJ]'+ z[obj_i[1]+12:]\n",
    "            #     else:\n",
    "            #         z= z[:obj_i[0]] + '[OBJ]'+ z[obj_i[0]: obj_i[1]+1]+ '[/OBJ]'+ z[obj_i[1]+1:]\n",
    "            #         z= z[:sub_i[0]+11] + '[SUB]'+ z[sub_i[0]+11: sub_i[1]+12] + '[/SUB]' + z[sub_i[1]+12:]\n",
    "\n",
    "            # elif self.version== 'PUN':\n",
    "            #     if sub_i[0] < obj_i[0]:\n",
    "            #         z= z[:sub_i[0]] + '@*'+sub_typ+'*'+ z[sub_i[0]: sub_i[1]+1] + '@' + z[sub_i[1]+1:]\n",
    "            #         z= z[:obj_i[0]+7] + '#^'+ obj_typ +'^'+ z[obj_i[0]+7: obj_i[1]+8]+ '#'+ z[obj_i[1]+8:]\n",
    "            #     else:\n",
    "            #         z= z[:obj_i[0]] + '#^'+ obj_typ +'^'+ z[obj_i[0]: obj_i[1]+1]+ '#' + z[obj_i[1]+1:]\n",
    "            #         z= z[:sub_i[0]+7] + '@*'+sub_typ+'*' + z[sub_i[0]+7: sub_i[1]+8] + '@' + z[sub_i[1]+8:]\n",
    "\n",
    "            sentence.append(z)\n",
    "\n",
    "        df= pd.DataFrame({'id': data['id'], 'sentence' : sentence, 'subject_entity': sub_entity, 'object_entity': obj_entity,\n",
    "                                'subject_type': sub_type, 'object_type': obj_type, 'label': data['label'],\n",
    "                                'subject_idx': sub_idx, 'object_idx': obj_idx})\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess= Preprocess(\"../dataset/train/train.csv\", 'SUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_dataset = preprocess.data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
