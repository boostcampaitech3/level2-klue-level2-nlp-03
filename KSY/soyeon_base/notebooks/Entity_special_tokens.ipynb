{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b73d034-bf29-4639-ac07-22d737104193",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7efaea-2eb7-4abd-a3fd-62da11f6ebe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 't', '##est', 'for', '[CLS]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_eng = AutoTokenizer.from_pretrained('roberta-large')\n",
    "tokenizer_eng.tokenize(\"this is test for [CLS]\")\n",
    "\n",
    "tokenizer_kor = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "tokenizer_kor.tokenize(\"this is test for [CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ef09b1-77c8-4ece-920e-e7cbff61d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Ġis', 'Ġtest', 'Ġfor', 'Ġ[', 'CL', 'S', ']']\n",
      "['this', 'is', 't', '##est', 'for', '[CLS]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_eng.tokenize(\"this is test for [CLS]\"))\n",
    "print(tokenizer_kor.tokenize(\"this is test for [CLS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd93d99-6fe9-4b95-bb6e-5484ef490da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/ml/level2-klue-level2-nlp-03/KSY')\n",
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9580ce32-38f2-4c33-a891-4e2aa6db7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/opt/ml/dataset/train/train.csv\"\n",
    "# original type data\n",
    "train_dataset = load_data(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46764b94-c6c1-4a0a-8ec2-2ef999bddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_ent_dataset(dataset):\n",
    "  # \"\"\" 처음 불러온 csv 파일에서 sentecen에 등장하고 start_idx, end_idx 를 갖는 subject, object를 marking.\"\"\"\n",
    "    subject_entity = []\n",
    "    object_entity = []\n",
    "    subject_type = []\n",
    "    object_type = []\n",
    "    # subject_entity : {'word':- , 'start_idx':-, 'end_idx':-}\n",
    "    for idx, (subj, obj, sents) in enumerate(zip(dataset['subject_entity'] ,dataset['object_entity'], dataset['sentence'])):\n",
    "        subj, obj = eval(subj), eval(obj)\n",
    "        new_subj = '[E1]' + sents[subj['start_idx'] : subj['end_idx']+1] + '[/E1]'\n",
    "        new_obj = '[E2]' + sents[obj['start_idx'] : obj['end_idx']+1] + '[/E2]'\n",
    "        if subj['start_idx'] >= obj['start_idx']:\n",
    "            temp = sents[:obj['start_idx']] + new_obj + sents[obj['end_idx']+1 : subj['start_idx']] + new_subj + sents[subj['end_idx']+1 :]\n",
    "        elif subj['start_idx'] < obj['start_idx']:\n",
    "            temp = sents[:subj['start_idx']] + new_subj + sents[subj['end_idx']+1 : obj['start_idx']] + new_obj + sents[obj['end_idx']+1 :]\n",
    "        \n",
    "        dataset['sentence'][idx] = temp\n",
    "        \n",
    "        subject_entity.append(subj['word'])\n",
    "        object_entity.append(obj['word'])\n",
    "        \n",
    "        subject_type.append(subj['type'])\n",
    "        object_type.append(obj['type'])\n",
    "\n",
    "    # print(len(dataset['id'], len(dataset['sentence']))\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],\n",
    "                                'subject_entity':subject_entity,'object_entity':object_entity,\n",
    "                                'subject_type':subject_type,'object_type':object_type,\n",
    "                                'label':dataset['label'],})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e8142b-85bb-4b7b-bafc-79e3178d1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_load_data(dataset_dir):\n",
    "  # \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_ent_dataset(pd_dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc80366-d781-4ec0-b6cc-6c13692de684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-9ebe380a9d21>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['sentence'][idx] = temp\n"
     ]
    }
   ],
   "source": [
    "en_train_data_dir = \"/opt/ml/dataset/train/train.csv\"\n",
    "ent_dataset = en_load_data(en_train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23bdf5f-2b86-4bd3-8825-693c6a672518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# bert-base-uncased & bert-base-cased\n",
    "sub_obj = [\"[E1]\", \"[/E1]\",\"[E2]\", \"[/E2]\"]\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\",\n",
    "                                         add_special_tokens = True)\n",
    "#dir(tokenizer)\n",
    "test_tokenizer.add_special_tokens({\n",
    "                            \"additional_special_tokens\": sub_obj\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e22c81-c2b5-4dd0-ba58-bfd613bac8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[CLS]',\n",
       " 'eos_token': '[SEP]',\n",
       " 'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85aaca15-6e84-460b-a377-d9862814a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer size\n",
    "print(len(test_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06ed8c9a-5b9d-434f-be1f-abc3da74f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "    # TODO! multi sentence ; single sentence\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "                          concat_entity,\n",
    "                          list(dataset['sentence']),\n",
    "                          return_tensors=\"pt\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=256,\n",
    "                          add_special_tokens=True,\n",
    "                          )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea12bf74-616c-407c-b92b-c1020b809e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset = tokenized_dataset(ent_dataset, test_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac40d95c-1adc-42ad-b7f9-b829f41540b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 29830,     2,  8373, 14113,  2234,     2,   168, 30985, 14451,\n",
       "         7088,  4586,   169,   793, 32002,  8373, 14113,  2234, 32003,  1504,\n",
       "         1363,  2088, 32000, 29830, 32001,   543, 14879,  2440,  6711,   170,\n",
       "        21406, 26713,  2076, 25145,  5749,   171,  1421,   818,  2073,  4388,\n",
       "         2062,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85b96a89-cd93-406c-af10-4de194937bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 민주 ##평 ##화 ##당 [SEP] 대안 ##신 ##당 [SEP] 호남 ##이 기반 ##인 바른 ##미 ##래 ##당 · [E2] 대안 ##신 ##당 [/E2] · [E1] 민주 ##평 ##화 ##당 [/E1] 이 우여곡절 끝 ##에 합당 ##해 민생 ##당 ( 가칭 ) 으로 재 ##탄 ##생 ##한다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "print(*out_dataset[1].tokens)\n",
    "print(*out_dataset[1].special_tokens_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f3af360-f265-48a1-ab6f-827deb791039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '민주', '##평', '##화', '##당', '[SEP]', '대안', '##신', '##당', '[SEP]', '호남', '##이', '기반', '##인', '바른', '##미', '##래', '##당', '·', '[E2]', '대안', '##신', '##당', '[/E2]', '·', '[E1]', '민주', '##평', '##화', '##당', '[/E1]', '이', '우여곡절', '끝', '##에', '합당', '##해', '민생', '##당', '(', '가칭', ')', '으로', '재', '##탄', '##생', '##한다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "out_list = out_dataset[1].tokens\n",
    "print(out_list)\n",
    "for idx, c in enumerate(out_list):\n",
    "    if c =='[PAD]':\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcc21777-af2f-4c73-a3b3-c14d8c97d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "d_data = out_dataset[1].attention_mask\n",
    "for idx, d in enumerate(d_data):\n",
    "    if d ==0:\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d5c3ad6-7040-4fe0-8212-032216d06cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dataset['token_type_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76d70457-0fcc-40fc-8cce-25a82f6fbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-base-uncased\n",
    "test_tokenizer2 = AutoTokenizer.from_pretrained(\"klue/bert-base\",\n",
    "                                         add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502fc86e-7971-4666-8c7a-1a437b45a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset = tokenized_dataset(ent_dataset, test_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6574325f-301e-4ccc-8fc4-65d8fdf06ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dataset['token_type_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e02f99-ae2c-4871-94d2-405250d52cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
