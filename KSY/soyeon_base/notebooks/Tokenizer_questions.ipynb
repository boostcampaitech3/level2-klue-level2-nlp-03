{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb89563a-47f5-4f84-99d4-b1b0cb207642",
   "metadata": {},
   "source": [
    "### Pretrained 가져오는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2b6036-0faf-46b9-aea4-f90f541571c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['such', 'a', 'scene']\n",
      "['ᄀ', '##ᅧ', '##ᆼ', '##ᄎ', '##ᅵ', '##ᄀ', '##ᅡ', 'ᄂ', '##ᅥ', '##ᄆ', '##ᅮ', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# bert-base-uncased & bert-base-cased\n",
    "\n",
    "sentences = '경치가 너무 좋네요'\n",
    "\n",
    "# 영어버전\n",
    "tokenizer_eng = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# 영어\n",
    "print(tokenizer_eng.tokenize(\"such a scene\"))\n",
    "# 한글\n",
    "print(tokenizer_eng.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a78516-dde0-42bd-a9e1-8994c14ebf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['su', '##ch', 'a', 'sc', '##ene']\n",
      "['경치', '##가', '너무', '좋', '##네', '##요']\n"
     ]
    }
   ],
   "source": [
    "# 한글버전\n",
    "tokenizer_kor = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# 영어\n",
    "print(tokenizer_kor.tokenize(\"such a scene\"))\n",
    "# 한글\n",
    "print(tokenizer_kor.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26544256-423e-4958-ae84-3ce525bac250",
   "metadata": {},
   "source": [
    "### CLS 토큰 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22bc5fc-960a-4fb4-910b-057f6cc8ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄀ', '##ᅧ', '##ᆼ', '##ᄎ', '##ᅵ', '##ᄀ', '##ᅡ', 'ᄂ', '##ᅥ', '##ᄆ', '##ᅮ', '[CLS]', '[UNK]']\n",
      "['경치', '##가', '너무', '[CLS]', '좋아', '##요']\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = '경치가 너무 [CLS] 좋아요'\n",
    "# 영어 tokenizer\n",
    "print(tokenizer_eng.tokenize(sample_sentences))\n",
    "\n",
    "# 한글 tokenizer\n",
    "print(tokenizer_kor.tokenize(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec825e-70e2-4b19-ab48-1daf869b690f",
   "metadata": {},
   "source": [
    "### 새로 학습 시키는 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b538e2e6-5c51-43bf-b3b6-0a9861afa206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./my_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize an empty tokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n",
    "    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼게버립니다.\n",
    "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
    "    lowercase=False,    # Hello -> hello\n",
    ")\n",
    "\n",
    "# And then train\n",
    "wp_tokenizer.train(\n",
    "    files=\"./data/wiki_20190620_small.txt\",\n",
    "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# Save the files\n",
    "wp_tokenizer.save_model(\"./\", \"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab37c2d-d715-48e8-974d-87e1d5eddfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open('./my_tokenizer-vocab.txt', 'r') as f:\n",
    "    vocabs = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06ab61da-cea7-4e33-89b0-4c0e898e5bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\"', '#', '%']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c4f631-49d6-4feb-be0a-b6dbf8da4d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['경', '##치가', '너무', '[CLS]', '좋아', '##요']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wp_tokenizer.tokenize(sample_sentences)\n",
    "wp_tokenizer.encode(sample_sentences).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c9c9af3-d063-48ee-977d-124db1c1c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "trained_tokenizer = BertTokenizerFast(\n",
    "    vocab_file='./my_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c188298-104b-41a0-80db-c284fb69607f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['경', '##치가', '너무', '[', 'C', '##L', '##S', ']', '좋아', '##요']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tokenizer.tokenize(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37cb5bcc-3206-4d26-9c8f-1f9c3bd6e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['경', '##치가', '너무', '[', 'C', '##L', '##S', ']', '좋아', '##요']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tokenizer2 = BertTokenizerFast(\n",
    "    vocab_file='./my_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=False,\n",
    "    add_special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    )\n",
    "trained_tokenizer2.tokenize(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b7b8f-522f-4bd5-914b-d0e2d2360cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc92f5-aaef-413f-a14b-fb0729a89922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
