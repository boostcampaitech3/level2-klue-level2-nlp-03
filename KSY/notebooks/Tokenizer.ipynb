{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13199af7-80e3-40d6-bd8d-dbdfc9566b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'new', 'gp', '##u', '!']\n",
      "['ᄂ', '##ᅡ', 'gp', '##u', '[UNK]', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# bert-base-uncased & bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 영어\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "# 한글\n",
    "print(tokenizer.tokenize(\"나 GPU 있다!!!\"))\n",
    "# “##”는 해당 심볼을 지닌 토큰은 해당 토큰 이전에 등장한 토큰과 공백 없이 합친다는 의미\n",
    "# subword tokenization : \n",
    "# 해당 어휘집(vocabulary)에 존재하는 토큰들을 얻을 수 있을 때까지 단어 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db21bc2-4e85-4e88-9fda-efb8f7c273a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'new', 'gp', '##u', '!']\n",
      "['ᄂ', '##ᅡ', 'gp', '##u', '[UNK]', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# 영어\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "# 한글\n",
    "print(tokenizer.tokenize(\"나 GPU 있다!!!\"))\n",
    "#  “##”는 해당 심볼을 지닌 토큰은 해당 토큰 이전에 등장한 토큰과 공백 없이 합쳐져야 한다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89f0581-8f62-4bc9-9da1-7ca5649430fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "# https://github.com/google/sentencepiece#installation\n",
    "# 혹시 install 하고 None 뜨면 노트북 restart 한번 해주셔야되네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765f698d-f3a8-4319-b98b-3137b1ca3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁have', '▁a', '▁new', '▁G', 'PU', '!']\n",
      "['▁', '나', '▁G', 'PU', '▁', '있다', '!!!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "print(tokenizer.tokenize(\"나 GPU 있다!!!\"))\n",
    "# sentencepiece에서 입력 문장을 Raw Stream으로 취급해 공백을 포함한 모든 캐릭터를 활용해, BPE 혹은 Unigram을 적용하며 사전을 구축하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699d0be-ecc1-45f8-8625-a05c6b808e01",
   "metadata": {},
   "source": [
    "### 토크나이저 중간과정 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22abe26b-8302-439c-9552-14d9ed0496ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize sentence :  ['Now', 'I', 'am', 'doing', 'Bo', '##ost', '##cam', '##p', 'AI', 'Tech', 'competition', '!', '!']\n",
      "give ids to token :  [1986, 146, 1821, 1833, 9326, 15540, 24282, 1643, 19016, 7882, 2208, 106, 106]\n",
      "Now I am doing Boostcamp AI Tech competition!!\n",
      "--\n",
      "[101, 1986, 146, 1821, 1833, 9326, 15540, 24282, 1643, 19016, 7882, 2208, 106, 106, 102]\n",
      "101 102 -> [CLS] [SEP]\n",
      "[CLS] Now I am doing Boostcamp AI Tech competition!! [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Now I am doing Boostcamp AI Tech competition!!\"\n",
    "\n",
    "# tokenize: vocabulary에 존재하는 토큰을 얻을 수 있을 때까지 단어 분리\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print('tokenize sentence : ',tokens)\n",
    "\n",
    "# 토큰들의 입력 식별자로 변환! 텐서로 변환되면 모델 입력으로 사용 가능\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('give ids to token : ',ids)\n",
    "\n",
    "# decoding: 인덱스를 다시 토큰으로 변환,하위 단어(subword)로 분할된 토큰을 병합\n",
    "# 원래의 읽을 수 있는 원본 문장으로 변환\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)\n",
    "\n",
    "print('--')\n",
    "\n",
    "# encoding 은 tokenize, convert to ids 두 과정으로 구성되며,\n",
    "# 시작을 표시하는 [CLS], 끝인 [SEP] 를 같이 표기해해서 줌\n",
    "enc_ids = tokenizer.encode(sequence)\n",
    "print(enc_ids)\n",
    "additionals = []\n",
    "for enc in enc_ids:\n",
    "    if enc not in ids:\n",
    "        additionals.append(enc)\n",
    "add_string = tokenizer.decode(additionals)      \n",
    "print(*additionals, '->', add_string)\n",
    "\n",
    "enc_string = tokenizer.decode(enc_ids)\n",
    "print(enc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3295103d-1e02-4a4f-adb7-be5fe4916f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = tokenizer.encode_batch(\n",
    "#    [[\"Hello, y'all!\", \"How are you 😁 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44d363b7-c511-45a9-a0af-89304493e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# pretrained BertWordPieceTokenizer에는 어떤 단어들이 있을까요?\n",
    "# 참고: https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#pretrained\n",
    "# 위의 예시처럼 간단히 from_pretrained 를 불러올 수도 있고\n",
    "from transformers import BertTokenizer\n",
    "sequences = [\"I really want to understand all!!\",\n",
    "            \"진짜로 다 이해하고 싶다!!\"]\n",
    "tokenizer_base = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer(sequences, return_tensors=\"pt\")\n",
    "# print(tokenizer.tokenize(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1755e1d-d06a-4528-8c8f-a0152f5dbf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'really', 'want', 'to', 'understand', 'all', '!', '!'], ['[UNK]', 'ᄃ', '##ᅡ', 'ᄋ', '##ᅵ', '##ᄒ', '##ᅢ', '##ᄒ', '##ᅡ', '##ᄀ', '##ᅩ', '[UNK]', '!', '!']]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.tokenize 가 string list 바로 먹혀지진 않네요. (!?)\n",
    "print([tokenizer_base.tokenize(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b7d56e-4b58-4d9f-a19b-38c81dde8b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PAD]          0\n",
       "[unused0]      1\n",
       "[unused1]      2\n",
       "[unused2]      3\n",
       "[unused3]      4\n",
       "[unused4]      5\n",
       "[unused5]      6\n",
       "[unused6]      7\n",
       "[unused7]      8\n",
       "[unused8]      9\n",
       "[unused9]     10\n",
       "[unused10]    11\n",
       "[unused11]    12\n",
       "[unused12]    13\n",
       "[unused13]    14\n",
       "[unused14]    15\n",
       "[unused15]    16\n",
       "[unused16]    17\n",
       "[unused17]    18\n",
       "[unused18]    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bert_uncased_vocab = pd.Series({k:v for k,v in tokenizer_base.get_vocab().items()})\n",
    "bert_uncased_vocab.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "850c4dc2-d042-4cd1-ad58-8a0a8bf28628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "# !apt-get update; apt-get install wget\n",
    "# !wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "# 해당 토크나이저가 갖는 vocabulary파일을 가지고 불러올 수도 있음\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77187481-6953-4f85-8969-f852a32d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizer와 BertWordPieceTokenizer 차이\n",
    "# https://stackoverflow.com/questions/62405155/bertwordpiecetokenizer-vs-berttokenizer-from-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65ca7d01-98e3-424e-9ce6-c26315d8058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2428, 2215, 2000, 3305, 2035, 999, 999, 102]\n",
      "[101, 100, 1457, 30006, 1463, 30019, 30005, 30007, 30005, 30006, 29991, 30011, 100, 999, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "output2 = [tokenizer_base.encode(seq) for seq in sequences]\n",
    "for o in output2:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9d24bf8-aed2-428e-9797-43b8d4e9bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained\n",
    "# https://github.com/huggingface/tokenizers/issues/507\n",
    "# https://github.com/huggingface/transformers/issues/6186\n",
    "new_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449a3d8-3dee-4502-970c-66859d89ed39",
   "metadata": {},
   "source": [
    "### word piece vocab 만들어보기\n",
    "* BertWordPieceTokenizer, BertTokenizerFast\n",
    "* 디테일한 argumnets 설명은 달지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc764771-af45-418c-a990-6e125bd78d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습에서 사용한 작은 wiki 로 사용\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
    "\n",
    "corpus_file   = './data/wiki_20190620_small.txt' # data path\n",
    "vocab_size    = 32000   #vocab의 크기. 보통 32,000이 좋다고 알려짐.\n",
    "limit_alphabet= 6000    #merge 수행 전 initial tokens이 유지되는 숫자 제한 (?!)\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "min_frequency = 5  # 단어의 최소 발생 빈도\n",
    "\n",
    "\n",
    "# BertTokenizer는 .train이 안된다..흠\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18170373-b0b8-41d0-b092-bba67dc048d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab으로 불러오는 방법! BertWordPieceTokenizer 는 config.json이 저장이 안된다.\n",
    "tokenizer.save_model(\n",
    "                    directory = './'\n",
    "                    )\n",
    "# import os\n",
    "# if os.path.exists('./wiki-bert'):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.mkdir('./wiki-bert')\n",
    "# tokenizer.save_model(\n",
    "#                     './wiki-bert','wiki-bert'\n",
    "#                     )\n",
    "wiki_bert = BertWordPieceTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c99bc988-7faa-497b-bca3-29b547a457cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'BertWordPieceTokenizer' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-a86baab8fa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwiki_bert2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertWordPieceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./wiki-bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BertWordPieceTokenizer' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "# BertWordPieceTokenizer from_pretrained 메소드도 없다\n",
    "wiki_bert2 = BertWordPieceTokenizer.from_pretrained('./wiki-bert') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f216099-1faa-4e8d-be6b-90e93dbad1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁', 'a', '▁', 't', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "                            \n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./data/wiki_20190620_small.txt\"], vocab_size=500, min_frequency=2)\n",
    "\n",
    "output = tokenizer.encode(\"This is a test\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ad3274ab-ee48-42bc-b376-79b186fde850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence-test/sentence-vocab.json', 'sentence-test/sentence-merges.txt']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('sentence-vocab.txt')\n",
    "import os\n",
    "if os.path.exists('sentence-test'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('sentence-test')\n",
    "tokenizer.save_model(\"sentence-test\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "30f1de0c-ea89-46c8-b71c-ce1b8e9afe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/tokenizers/blob/master/bindings/python/py_src/tokenizers/implementations/sentencepiece_bpe.py\n",
    "wiki_bert = SentencePieceBPETokenizer(\n",
    "    vocab = './sentence-test/sentence-vocab.json',\n",
    "    merges = './sentence-test/sentence-merges.txt') # config file이 있어줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "68823418-72a8-4788-be40-c53f9168e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [2, 76, 85, 5599, 1896, 1896, 1865, 90, 8657, 87, 1749, 88, 6376, 3246, 1866, 1962, 4115, 1898, 68, 1896, 1896, 5, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [2, 1190, 2091, 1575, 493, 3345, 2704, 967, 1586, 5, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# BertTokenizer, BertWordPieceTokenizer\n",
    "# BertWordPieceTokenizer, BertTokenizerFast의 차이\n",
    "from transformers import BertTokenizerFast\n",
    "# https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/tokenization_bert_fast.py#L117\n",
    "# Construct a \"fast\" BERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
    "#    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
    "#    refer to this superclass for more information regarding those methods.\n",
    "tokenizer = BertTokenizerFast(\"./vocab.txt\",\n",
    "                             strip_accents=False,\n",
    "                             lowercase=False)\n",
    "print([tokenizer(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "13fbfabd-ea74-4bcc-b8ec-afe24ed19639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizerFast -> train, save model 다 안됨. 그냥 불러오는 것만 되는듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcac1c-784d-4f9d-8af9-501ab4d1f95b",
   "metadata": {},
   "source": [
    "### pipeline 사용하기\n",
    "* 간단한 예제 : `senteiment-analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000af2e-cf5f-4847-8121-9586ec70f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
