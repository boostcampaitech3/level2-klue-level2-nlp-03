{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13199af7-80e3-40d6-bd8d-dbdfc9566b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'new', 'gp', '##u', '!']\n",
      "['á„‚', '##á…¡', 'gp', '##u', '[UNK]', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# bert-base-uncased & bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì˜ì–´\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "# í•œê¸€\n",
    "print(tokenizer.tokenize(\"ë‚˜ GPU ìˆë‹¤!!!\"))\n",
    "# â€œ##â€ëŠ” í•´ë‹¹ ì‹¬ë³¼ì„ ì§€ë‹Œ í† í°ì€ í•´ë‹¹ í† í° ì´ì „ì— ë“±ì¥í•œ í† í°ê³¼ ê³µë°± ì—†ì´ í•©ì¹œë‹¤ëŠ” ì˜ë¯¸\n",
    "# subword tokenization : \n",
    "# í•´ë‹¹ ì–´íœ˜ì§‘(vocabulary)ì— ì¡´ì¬í•˜ëŠ” í† í°ë“¤ì„ ì–»ì„ ìˆ˜ ìˆì„ ë•Œê¹Œì§€ ë‹¨ì–´ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db21bc2-4e85-4e88-9fda-efb8f7c273a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'new', 'gp', '##u', '!']\n",
      "['á„‚', '##á…¡', 'gp', '##u', '[UNK]', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# ì˜ì–´\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "# í•œê¸€\n",
    "print(tokenizer.tokenize(\"ë‚˜ GPU ìˆë‹¤!!!\"))\n",
    "#  â€œ##â€ëŠ” í•´ë‹¹ ì‹¬ë³¼ì„ ì§€ë‹Œ í† í°ì€ í•´ë‹¹ í† í° ì´ì „ì— ë“±ì¥í•œ í† í°ê³¼ ê³µë°± ì—†ì´ í•©ì³ì ¸ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89f0581-8f62-4bc9-9da1-7ca5649430fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "# https://github.com/google/sentencepiece#installation\n",
    "# í˜¹ì‹œ install í•˜ê³  None ëœ¨ë©´ ë…¸íŠ¸ë¶ restart í•œë²ˆ í•´ì£¼ì…”ì•¼ë˜ë„¤ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765f698d-f3a8-4319-b98b-3137b1ca3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–I', 'â–have', 'â–a', 'â–new', 'â–G', 'PU', '!']\n",
      "['â–', 'ë‚˜', 'â–G', 'PU', 'â–', 'ìˆë‹¤', '!!!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "print(tokenizer.tokenize(\"I have a new GPU!\"))\n",
    "print(tokenizer.tokenize(\"ë‚˜ GPU ìˆë‹¤!!!\"))\n",
    "# sentencepieceì—ì„œ ì…ë ¥ ë¬¸ì¥ì„ Raw Streamìœ¼ë¡œ ì·¨ê¸‰í•´ ê³µë°±ì„ í¬í•¨í•œ ëª¨ë“  ìºë¦­í„°ë¥¼ í™œìš©í•´, BPE í˜¹ì€ Unigramì„ ì ìš©í•˜ë©° ì‚¬ì „ì„ êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699d0be-ecc1-45f8-8625-a05c6b808e01",
   "metadata": {},
   "source": [
    "### í† í¬ë‚˜ì´ì € ì¤‘ê°„ê³¼ì • í™•ì¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22abe26b-8302-439c-9552-14d9ed0496ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize sentence :  ['Now', 'I', 'am', 'doing', 'Bo', '##ost', '##cam', '##p', 'AI', 'Tech', 'competition', '!', '!']\n",
      "give ids to token :  [1986, 146, 1821, 1833, 9326, 15540, 24282, 1643, 19016, 7882, 2208, 106, 106]\n",
      "Now I am doing Boostcamp AI Tech competition!!\n",
      "--\n",
      "[101, 1986, 146, 1821, 1833, 9326, 15540, 24282, 1643, 19016, 7882, 2208, 106, 106, 102]\n",
      "101 102 -> [CLS] [SEP]\n",
      "[CLS] Now I am doing Boostcamp AI Tech competition!! [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Now I am doing Boostcamp AI Tech competition!!\"\n",
    "\n",
    "# tokenize: vocabularyì— ì¡´ì¬í•˜ëŠ” í† í°ì„ ì–»ì„ ìˆ˜ ìˆì„ ë•Œê¹Œì§€ ë‹¨ì–´ ë¶„ë¦¬\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print('tokenize sentence : ',tokens)\n",
    "\n",
    "# í† í°ë“¤ì˜ ì…ë ¥ ì‹ë³„ìë¡œ ë³€í™˜! í…ì„œë¡œ ë³€í™˜ë˜ë©´ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('give ids to token : ',ids)\n",
    "\n",
    "# decoding: ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ í† í°ìœ¼ë¡œ ë³€í™˜,í•˜ìœ„ ë‹¨ì–´(subword)ë¡œ ë¶„í• ëœ í† í°ì„ ë³‘í•©\n",
    "# ì›ë˜ì˜ ì½ì„ ìˆ˜ ìˆëŠ” ì›ë³¸ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)\n",
    "\n",
    "print('--')\n",
    "\n",
    "# encoding ì€ tokenize, convert to ids ë‘ ê³¼ì •ìœ¼ë¡œ êµ¬ì„±ë˜ë©°,\n",
    "# ì‹œì‘ì„ í‘œì‹œí•˜ëŠ” [CLS], ëì¸ [SEP] ë¥¼ ê°™ì´ í‘œê¸°í•´í•´ì„œ ì¤Œ\n",
    "enc_ids = tokenizer.encode(sequence)\n",
    "print(enc_ids)\n",
    "additionals = []\n",
    "for enc in enc_ids:\n",
    "    if enc not in ids:\n",
    "        additionals.append(enc)\n",
    "add_string = tokenizer.decode(additionals)      \n",
    "print(*additionals, '->', add_string)\n",
    "\n",
    "enc_string = tokenizer.decode(enc_ids)\n",
    "print(enc_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3295103d-1e02-4a4f-adb7-be5fe4916f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = tokenizer.encode_batch(\n",
    "#    [[\"Hello, y'all!\", \"How are you ğŸ˜ ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44d363b7-c511-45a9-a0af-89304493e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# pretrained BertWordPieceTokenizerì—ëŠ” ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ìˆì„ê¹Œìš”?\n",
    "# ì°¸ê³ : https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#pretrained\n",
    "# ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ê°„ë‹¨íˆ from_pretrained ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆê³ \n",
    "from transformers import BertTokenizer\n",
    "sequences = [\"I really want to understand all!!\",\n",
    "            \"ì§„ì§œë¡œ ë‹¤ ì´í•´í•˜ê³  ì‹¶ë‹¤!!\"]\n",
    "tokenizer_base = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer(sequences, return_tensors=\"pt\")\n",
    "# print(tokenizer.tokenize(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1755e1d-d06a-4528-8c8f-a0152f5dbf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'really', 'want', 'to', 'understand', 'all', '!', '!'], ['[UNK]', 'á„ƒ', '##á…¡', 'á„‹', '##á…µ', '##á„’', '##á…¢', '##á„’', '##á…¡', '##á„€', '##á…©', '[UNK]', '!', '!']]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.tokenize ê°€ string list ë°”ë¡œ ë¨¹í˜€ì§€ì§„ ì•Šë„¤ìš”. (!?)\n",
    "print([tokenizer_base.tokenize(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b7d56e-4b58-4d9f-a19b-38c81dde8b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PAD]          0\n",
       "[unused0]      1\n",
       "[unused1]      2\n",
       "[unused2]      3\n",
       "[unused3]      4\n",
       "[unused4]      5\n",
       "[unused5]      6\n",
       "[unused6]      7\n",
       "[unused7]      8\n",
       "[unused8]      9\n",
       "[unused9]     10\n",
       "[unused10]    11\n",
       "[unused11]    12\n",
       "[unused12]    13\n",
       "[unused13]    14\n",
       "[unused14]    15\n",
       "[unused15]    16\n",
       "[unused16]    17\n",
       "[unused17]    18\n",
       "[unused18]    19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bert_uncased_vocab = pd.Series({k:v for k,v in tokenizer_base.get_vocab().items()})\n",
    "bert_uncased_vocab.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "850c4dc2-d042-4cd1-ad58-8a0a8bf28628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "# !apt-get update; apt-get install wget\n",
    "# !wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "# í•´ë‹¹ í† í¬ë‚˜ì´ì €ê°€ ê°–ëŠ” vocabularyíŒŒì¼ì„ ê°€ì§€ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŒ\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77187481-6953-4f85-8969-f852a32d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizerì™€ BertWordPieceTokenizer ì°¨ì´\n",
    "# https://stackoverflow.com/questions/62405155/bertwordpiecetokenizer-vs-berttokenizer-from-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65ca7d01-98e3-424e-9ce6-c26315d8058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2428, 2215, 2000, 3305, 2035, 999, 999, 102]\n",
      "[101, 100, 1457, 30006, 1463, 30019, 30005, 30007, 30005, 30006, 29991, 30011, 100, 999, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "output2 = [tokenizer_base.encode(seq) for seq in sequences]\n",
    "for o in output2:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9d24bf8-aed2-428e-9797-43b8d4e9bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained\n",
    "# https://github.com/huggingface/tokenizers/issues/507\n",
    "# https://github.com/huggingface/transformers/issues/6186\n",
    "new_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449a3d8-3dee-4502-970c-66859d89ed39",
   "metadata": {},
   "source": [
    "### word piece vocab ë§Œë“¤ì–´ë³´ê¸°\n",
    "* BertWordPieceTokenizer, BertTokenizerFast\n",
    "* ë””í…Œì¼í•œ argumnets ì„¤ëª…ì€ ë‹¬ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc764771-af45-418c-a990-6e125bd78d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ ì‘ì€ wiki ë¡œ ì‚¬ìš©\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
    "\n",
    "corpus_file   = './data/wiki_20190620_small.txt' # data path\n",
    "vocab_size    = 32000   #vocabì˜ í¬ê¸°. ë³´í†µ 32,000ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì§.\n",
    "limit_alphabet= 6000    #merge ìˆ˜í–‰ ì „ initial tokensì´ ìœ ì§€ë˜ëŠ” ìˆ«ì ì œí•œ (?!)\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "min_frequency = 5  # ë‹¨ì–´ì˜ ìµœì†Œ ë°œìƒ ë¹ˆë„\n",
    "\n",
    "\n",
    "# BertTokenizerëŠ” .trainì´ ì•ˆëœë‹¤..í \n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet, \n",
    "               show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18170373-b0b8-41d0-b092-bba67dc048d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•! BertWordPieceTokenizer ëŠ” config.jsonì´ ì €ì¥ì´ ì•ˆëœë‹¤.\n",
    "tokenizer.save_model(\n",
    "                    directory = './'\n",
    "                    )\n",
    "# import os\n",
    "# if os.path.exists('./wiki-bert'):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.mkdir('./wiki-bert')\n",
    "# tokenizer.save_model(\n",
    "#                     './wiki-bert','wiki-bert'\n",
    "#                     )\n",
    "wiki_bert = BertWordPieceTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c99bc988-7faa-497b-bca3-29b547a457cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'BertWordPieceTokenizer' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-a86baab8fa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwiki_bert2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertWordPieceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./wiki-bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BertWordPieceTokenizer' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "# BertWordPieceTokenizer from_pretrained ë©”ì†Œë“œë„ ì—†ë‹¤\n",
    "wiki_bert2 = BertWordPieceTokenizer.from_pretrained('./wiki-bert') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f216099-1faa-4e8d-be6b-90e93dbad1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'T', 'h', 'i', 's', 'â–', 'i', 's', 'â–', 'a', 'â–', 't', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "                            \n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./data/wiki_20190620_small.txt\"], vocab_size=500, min_frequency=2)\n",
    "\n",
    "output = tokenizer.encode(\"This is a test\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ad3274ab-ee48-42bc-b376-79b186fde850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence-test/sentence-vocab.json', 'sentence-test/sentence-merges.txt']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('sentence-vocab.txt')\n",
    "import os\n",
    "if os.path.exists('sentence-test'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('sentence-test')\n",
    "tokenizer.save_model(\"sentence-test\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "30f1de0c-ea89-46c8-b71c-ce1b8e9afe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/tokenizers/blob/master/bindings/python/py_src/tokenizers/implementations/sentencepiece_bpe.py\n",
    "wiki_bert = SentencePieceBPETokenizer(\n",
    "    vocab = './sentence-test/sentence-vocab.json',\n",
    "    merges = './sentence-test/sentence-merges.txt') # config fileì´ ìˆì–´ì¤˜ì•¼í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "68823418-72a8-4788-be40-c53f9168e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [2, 76, 85, 5599, 1896, 1896, 1865, 90, 8657, 87, 1749, 88, 6376, 3246, 1866, 1962, 4115, 1898, 68, 1896, 1896, 5, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [2, 1190, 2091, 1575, 493, 3345, 2704, 967, 1586, 5, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# BertTokenizer, BertWordPieceTokenizer\n",
    "# BertWordPieceTokenizer, BertTokenizerFastì˜ ì°¨ì´\n",
    "from transformers import BertTokenizerFast\n",
    "# https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/tokenization_bert_fast.py#L117\n",
    "# Construct a \"fast\" BERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
    "#    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
    "#    refer to this superclass for more information regarding those methods.\n",
    "tokenizer = BertTokenizerFast(\"./vocab.txt\",\n",
    "                             strip_accents=False,\n",
    "                             lowercase=False)\n",
    "print([tokenizer(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "13fbfabd-ea74-4bcc-b8ec-afe24ed19639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizerFast -> train, save model ë‹¤ ì•ˆë¨. ê·¸ëƒ¥ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒë§Œ ë˜ëŠ”ë“¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcac1c-784d-4f9d-8af9-501ab4d1f95b",
   "metadata": {},
   "source": [
    "### pipeline ì‚¬ìš©í•˜ê¸°\n",
    "* ê°„ë‹¨í•œ ì˜ˆì œ : `senteiment-analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000af2e-cf5f-4847-8121-9586ec70f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
