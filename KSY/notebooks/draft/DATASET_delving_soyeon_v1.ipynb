{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f367e69e-cb1d-4db4-9384-f48d990fa03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "033c7e2a-cf64-4e8a-93f1-aba1be1bd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/opt/ml/level2-klue-level2-nlp-03/baseline/dataset/train/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "619ea670-7ff7-4417-b106-03d41a855f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cfefe40-5f45-4900-a2ec-07019e50c379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>'비틀즈'</td>\n",
       "      <td>'조지 해리슨'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>'민주평화당'</td>\n",
       "      <td>'대안신당'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>'광주FC'</td>\n",
       "      <td>'한국프로축구연맹'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>'아성다이소'</td>\n",
       "      <td>'박정부'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>'요미우리 자이언츠'</td>\n",
       "      <td>'1967'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_entity  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          '비틀즈'   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...        '민주평화당'   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...         '광주FC'   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...        '아성다이소'   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...    '요미우리 자이언츠'   \n",
       "\n",
       "  object_entity                      label  \n",
       "0      '조지 해리슨'                no_relation  \n",
       "1        '대안신당'                no_relation  \n",
       "2    '한국프로축구연맹'              org:member_of  \n",
       "3         '박정부'  org:top_members/employees  \n",
       "4        '1967'                no_relation  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cbfe97bc-cd67-4f7b-9f7c-e716eba952e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          id                                           sentence  \\\n",
       " 6667    6667  최창권(崔彰權, 천주교 세례명 바오로, 1934년 10월 26일 ~ 2008년 1월...   \n",
       " 24635  24635  1991년부터 1995년까지 클리블랜드 인디언스의 미국 동부 및 라틴 아메리카 담당...   \n",
       " 2934    2934  AFC 챔피언스리그 2011 일본 클럽 가시마 앤틀러스와의 원정경기에서 프로무대 첫...   \n",
       " 17968  17968  곡성군이 지난 14일부터 17일까지 유근기 군수 주재로 부군수, 부서장, 부서별 전...   \n",
       " 8739    8739  삼성전자는 한국에너지공단에서 시행하는 ‘으뜸효율 가전제품 구매비용 환급사업’에 맞춰...   \n",
       " ...      ...                                                ...   \n",
       " 16230  16230  《테일즈런너》는 대한민국의 라온 엔터테인먼트가 개발, 2005년에 출시된 캐주얼 온...   \n",
       " 25707  25707  현대 학자들은 그리스 정교회가 그리스인의 민족적 정체성을 지키고, 그리스 사회가 발...   \n",
       " 21968  21968  김영록 전라남도지사는 “각종 취약시설에 대해 코호트 수준으로 관리를 강화하고, 필요...   \n",
       " 27476  27476  5년동안의 극렬한 무력항쟁을 위해 의열단 지도부로써 거사를 기획하고, 단원확충과 교...   \n",
       " 2422    2422                        김원효·심진화 부부는 지난 2011년에 결혼했다.   \n",
       " \n",
       "       subject_entity object_entity                      label  \n",
       " 6667           '최창권'         '崔彰權'        per:alternate_names  \n",
       " 24635   '클리블랜드 인디언스'        '1995'                no_relation  \n",
       " 2934      '가시마 앤틀러스'        '2011'                no_relation  \n",
       " 17968          '곡성군'         '유근기'  org:top_members/employees  \n",
       " 8739          '삼성전자'        '가전제품'                org:product  \n",
       " ...              ...           ...                        ...  \n",
       " 16230    '라온 엔터테인먼트'        '대한민국'  org:place_of_headquarters  \n",
       " 25707          '그리스'         '정교회'                org:members  \n",
       " 21968          '김영록'      '전라남도지사'                  per:title  \n",
       " 27476          '의열단'         '김원봉'  org:top_members/employees  \n",
       " 2422           '김원효'         '심진화'                 per:spouse  \n",
       " \n",
       " [25976 rows x 5 columns],\n",
       "           id                                           sentence  \\\n",
       " 25362  25362  부산, 경남권에서는 2016년 5월 10일부터 KNN 부산경남방송과 제휴하여 KNN...   \n",
       " 9566    9566  그러던 중 1990년 삼당합당을 거부한 이기택이 주도하는 민주당(소위 꼬마민주당)에...   \n",
       " 11421  11421  루도비쿠스 2세 이우니오르(825년 11월 1일 – 875년 8월 12일)는 845...   \n",
       " 16395  16395  육군사관학교 동기인 김재춘이 중정 부장을 맡았을 때 차장으로 근무하면서 함경도 인맥...   \n",
       " 12665  12665            또 폐비 유씨, 효현왕후, 효정왕후 등 왕비들이 거처한 곳이기도 하다.   \n",
       " ...      ...                                                ...   \n",
       " 18063  18063  김요협은 1833년 절충장군 중추부첨지사를 지내고 사후 증 가선대부 내부협판에 추증...   \n",
       " 2699    2699  이어지는 38년간 프랑스와 아디다스는 돈독한 파트너십을 구축하였는데, 이 기간동안 ...   \n",
       " 10924  10924  1990년, 노태우 정부는 통일민주당의 김영삼 총재와 신민주공화당의 김종필 총재와의...   \n",
       " 20985  20985                 장윤정·도경완 아들이 말 한 마디로 모두를 뭉클하게 만들었다.   \n",
       " 19975  19975  저우언라이는 중국 장쑤 성 화이안에서 하급 관료의 아들로 태어나, 어렸을 때 5촌 ...   \n",
       " \n",
       "           subject_entity    object_entity                      label  \n",
       " 25362           '부산경남방송'            'KNN'        org:alternate_names  \n",
       " 9566               '민주당'            '김대중'                no_relation  \n",
       " 11421   '루도비쿠스 2세 이우니오르'    '875년 8월 12일'          per:date_of_death  \n",
       " 16395              '김재춘'         '육군사관학교'       per:schools_attended  \n",
       " 12665             '효현왕후'           '효정왕후'                no_relation  \n",
       " ...                  ...              ...                        ...  \n",
       " 18063              '김요협'          '1833년'          per:date_of_birth  \n",
       " 2699               '프랑스'   'UEFA 유로 1984'              org:member_of  \n",
       " 10924            '통일민주당'            '김영삼'  org:top_members/employees  \n",
       " 20985              '장윤정'            '도경완'                 per:spouse  \n",
       " 19975            '저우언라이'           '덩잉차오'                 per:spouse  \n",
       " \n",
       " [6494 rows x 5 columns]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label = train_dataset['label']\n",
    "train_test_split(train_dataset, test_size=0.2, shuffle=True, stratify=label, random_state=424)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae8c53-c249-43ac-b152-3acd017a368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label = train_dataset['label']\n",
    "train_test_split(train_dataset, test_size=0.2, shuffle=True, stratify=label, random_state=424)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e34df5-33ab-400f-9580-84928fdf7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for itr, vals in train_dataset.iterrows():\n",
    "#     if vals['label'] == 'org:alternate_names':\n",
    "#         print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e4fedf24-2ede-418b-9fe6-5787f5853a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0c3d62c-9335-4f31-a9d1-7f39e9a2ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_entity = []\n",
    "for e01, e02 in zip(train_dataset['subject_entity'], train_dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "\n",
    "tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(train_dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "55270ecc-a8b3-422e-9747-eb083b44bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 87, 2013, 3, 25419, 3]], 'token_type_ids': [[0, 0, 0, 0, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] tt [SEP] this [SEP]'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = tokenizer(['tt'],['this'])\n",
    "print(o)\n",
    "tokenizer.decode(o['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5679e8d0-ed11-4ccd-8a76-944304ec886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "##t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tt'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode([87]))\n",
    "print(tokenizer.decode([2013]))\n",
    "tokenizer.decode([87,2013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "13297b21-1cb1-4e91-bd9b-e60cc46b64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "30202e60-2fb5-4196-8763-cf07321f9ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    11, 29830,  ...,     0,     0,     0],\n",
       "        [    2,    11,  3772,  ...,     0,     0,     0],\n",
       "        [    2,    11,  4104,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    11, 18272,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15710,  ...,     0,     0,     0],\n",
       "        [    2,    11, 15437,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18360429-c517-4610-9ee2-0e54bc4c5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS]'비틀즈'[SEP]'조지 해리슨'[SEP] 〈 Something 〉 는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sentences['input_ids'][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "287962b1-ffc7-46c2-8d62-e7c5f89ce3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('./dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f228899-44f3-4c77-864f-03da709cd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "RE_train_dataset = RE_Dataset(tokenized_sentences, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77919d0f-b23d-4e18-8cc5-497150efd672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3da03e6c-d357-4640-b8e4-87dc825a31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenized_sentences['input_ids'][0])\n",
    "# print(tokenized_sentences['token_type_ids'][0])\n",
    "# tokenizer.decode(tokenized_sentences['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "939881d5-30cf-472b-9577-284e175fcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenized sentence가 머지? 왜 다 첫번째 sentence 표기값은 없지..\n",
    "# for i in range(len(tokenized_sentences)):\n",
    "#     print(tokenized_sentences['token_type_ids'][i])\n",
    "#     if i ==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "789654e8-e655-4939-a535-71b4e6ee19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a15d2175-4d79-49e3-99ac-f3bb3c1c138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RE_train_dataset[0]\n",
    "test.pop('labels')\n",
    "for k,v in test.items():\n",
    "    test[k]=v.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "26c0d154-5e0c-42b4-a110-8cb8e351de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a8ae5f9-1c43-4f05-83ac-b83cbb24d19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d5b9f57-5e95-46f1-9b78-cb5a7c463344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    11, 29830,    11,     3,    11,  8373, 14113,  2234,    11,\n",
       "              3,   168, 30985, 14451,  7088,  4586,   169,   793,  8373, 14113,\n",
       "           2234,  2052,  1363,  2088, 29830,  2116, 14879,  2440,  6711,   170,\n",
       "          21406, 26713,  2076, 25145,  5749,   171,  1421,   818,  2073,  4388,\n",
       "           2062,    18,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0]])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "19b2eee1-3c0e-4fc5-960c-1bfea11b6c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2,   50, 2237, 2325, 2259, 3944, 6001, 2051,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tokenizer(\"NLP는 정말 재미있어\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae535ad0-2559-4118-b962-9b0f714e8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8ee395e-7950-46f7-8be0-52989896a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "flag= False\n",
    "for v in tokenized_sentences['token_type_ids'][0]:\n",
    "    if v ==0 and not flag:\n",
    "        cnt += 1\n",
    "        flag = True\n",
    "        break\n",
    "    elif v!=0 and flag:\n",
    "        cnt += 1\n",
    "    elif v ==0 and flag:\n",
    "        break\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f7de07a-62d8-496a-a5c5-8856a2f8c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,    11,  3772,  2139,  2267,  2481,    11,     3,    11,  5605,\n",
      "         2250,  2481,    11,     3,  6409,  2052,  4568,  2179,  6417,  2044,\n",
      "         2315,  2481,   100,  5605,  2250,  2481,   100,  3772,  2139,  2267,\n",
      "         2481,  2052, 16489,   711,  2170, 12827,  2097,  8646,  2481,    12,\n",
      "        15283,    13,  3603,  1528,  2554,  2065,  4538,    18,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[CLS]'민주평화당'[SEP]'대안신당'[SEP] 호남이 기반인 바른미래당 · 대안신당 · 민주평화당이 우여곡절 끝에 합당해 민생당 ( 가칭 ) 으로 재탄생한다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_sentences['input_ids'][1])\n",
    "print(tokenized_sentences['token_type_ids'][1])\n",
    "tokenizer.decode(tokenized_sentences['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10994f49-c83f-4217-bf5f-ee8f2f933be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32470, 241])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feaa1c-0aed-46dd-b72f-54fce4f6bbbb",
   "metadata": {},
   "source": [
    "# 튜토리얼 따라한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35b725-6dd9-46ac-9a40-b160930733de",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e784a-c5e0-4bf8-be23-5e2d31627261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 궁금증\n",
    "# pretrained를 가지고 오면\n",
    "# tokenizer.tokenize 할 때 example의 tokenize 결과 나오는게 맞지?\n",
    "\n",
    "# tokenize 따로 train 시키는 법!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cb9d4-f1d6-4cc8-a1dc-7230ad362e97",
   "metadata": {},
   "source": [
    "* summary\n",
    "    * training data의 언어를 이해할 수 있는 것 사용\n",
    "    * 사용하는 pretrained model과 동일한 tokenizer인지 확인 -> vocab size mismath, special token이 unk로 처리될 수 있음\n",
    "        * 단어의 개수와 special token이 완전히 일치하는 모델은 드물다! -> 각 모델에 맞는 것 꼭 불러오자\n",
    "        * [Q] 그럼 finetuning할 때는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9303228-98c4-493f-b525-ad6ee0fcf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import FloatProgress\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "example = \"나는 학생이다.\"\n",
    "# tokenization 결과를 보여줍니다.\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb295c7-9fb0-44f6-b6da-d9390cfe4658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization 결과 :  ['[UNK]', '[UNK]', '.']\n",
      "tokenization + encoding 결과 :  [101, 100, 100, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "# encoding이 어떻게 되는건지 모르겠다.\n",
    "print('tokenization 결과 : ', tokenizer.tokenize(example))\n",
    "print('tokenization + encoding 결과 : ', tokenizer.encode(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b008714f-5934-482f-800e-c79b8d354399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]']\n",
      "{'input_ids': [2, 2, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "['[CLS]']\n",
      "{'input_ids': [2, 2, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# print('tokenization 결과 : ', tokenizer.tokenize(example))\n",
    "# print('tokenization + encoding 결과 : ', tokenizer.encode(example))\n",
    "print(tokenizer.tokenize(\"[CLS]\"))\n",
    "print(tokenizer(\"[CLS]\"))\n",
    "tokenizer.add_special_tokens({'cls_token':'[CLS]'})\n",
    "print(tokenizer.tokenize(\"[CLS]\"))\n",
    "print(tokenizer(\"[CLS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fc8ecf-23c1-47c9-a537-95ac09b9e740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7db90541029463ab146116e73cd3635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57281beea1e494fae214bbd914a8077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b61f5eccf8a43d99397ac120043a307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f984f0c7a14b8faf26aeb373c8892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a7af83-f7be-4fe7-a660-f20ac050e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]']\n",
      "{'input_ids': [101, 101, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "['[CLS]']\n",
      "{'input_ids': [101, 101, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"[CLS]\"))\n",
    "print(tokenizer(\"[CLS]\"))\n",
    "tokenizer.add_special_tokens({'cls_token':'[CLS]'})\n",
    "print(tokenizer.tokenize(\"[CLS]\"))\n",
    "print(tokenizer(\"[CLS]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84def256-deb5-4614-8545-4be255b9879a",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0aaf45-5d9d-4cd4-abc3-53175b336079",
   "metadata": {},
   "source": [
    "* summary\n",
    "    * 사전 학습 모델이 가진 셋팅을 따르자\n",
    "        * ㅁ델마다 vocab size, hidden dimension 등 파라미터 세팅이 다르다!\n",
    "    * config update 방법\n",
    "        * 호출 후 config attribute 으로 수정: `model_config.vocab_size = model_config.vocab_size+2`\n",
    "        * 호출하면서 동시에 함수 아규먼트 값으로 넣어주는 방법 : `AutoConfig.from_pretrained(model_name, vocab_size=32002`\n",
    "            * 이럴 경우 기존 config에 없는 값을 넣어주면 학습에 사용되지 않기 때문에 업데이트가 안되는 것 같다.\n",
    "        * 커스텀 config 정의 후 업데이트 :\n",
    "        ```\n",
    "        custom_config = {'vocab_size':32000}\n",
    "        model_config = AutoConfig.from_pretrained(model_name)\n",
    "        model_config.update(custom_config)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f679cc77-2993-4733-baf8-538dd794f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_name =  'klue/bert-base'\n",
    "# pretrained 모델과 동일한 configuration을 가져옵니다.\n",
    "model_config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83c5bc9-cde8-4ede-9601-fd2d7ba8b8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ed82a-5677-4f5a-899a-aa311a66d7f2",
   "metadata": {},
   "source": [
    "### pretrain model 불러오기\n",
    "* summary\n",
    "    * `from_config()` : config대로 모델 가져오는 것. pretrained weight 가져오는게 아님!\n",
    "    * `from_pretrained()` : model config에 해당하는 모델과 pretrained weight 가져옴!\n",
    "        * 스스로 학습한 모델은 model_name에 model이 저장된 directory 입력\n",
    "    * 모델 가져오는 옵션\n",
    "        * 기본 모델 : hidden state 출력되는 기본 모델\n",
    "        * down stream task 모델 : 일반적인 task를 쉽게 수행할 수 있게 미리 기본 모델 + head 설정 모델\n",
    "            * output은 task에 적합한 dimension으로 미리 저장돼있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd8042f1-adff-448a-862e-9f05de03bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForQuestionAnswering\n",
    "model_name =  'klue/bert-base'\n",
    "\n",
    "# pretrained 모델과 동일한 configuration을 가져옵니다.\n",
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# 모델을 정의합니다.\n",
    "\n",
    "# option 1 : config에서 정의한 모델을 가져오기 (initial)\n",
    "# model = AutoModelForQuestionAnswering.from_config(config)\n",
    "\n",
    "# option 2 : config에서 정의한 사전학습된 모델을 가져오기 (pretrained)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_name, config=model_config\n",
    "    )\n",
    "# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4322f41-c7af-4eb1-a141-dbe21b1db6fe",
   "metadata": {},
   "source": [
    "### 전체 학습시 선언 필요한 것\n",
    "* model_name 선언\n",
    "* model이 사용한 tokenizer\n",
    "* model이 사용한 config\n",
    "* model 불러오기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b831befd-1989-4e4b-814f-0009425b8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f88750-fe76-4d25-9359-c34ed35b1cce",
   "metadata": {},
   "source": [
    "# 모델을 학습해보자\n",
    "     * [참고](https://github.com/yukyunglee/transformers-resources/blob/master/tutorial/02%20advanced%20tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380c5697-ac83-4ab5-b7d8-39aabbf9ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(\"NLP는 정말 재미있어\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "cls_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5b9057-8754-4358-b58a-17f5983700e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e868a470-afd1-4037-b3f0-a5bb58f85f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feff6933-e5f3-4722-941f-1760c4603897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   50, 2237, 2325, 2259, 3944, 6001, 2051,    3]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e24dab5-fc2c-43a0-ba70-edf22e57c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] NLP는 정말 재미있어 [SEP]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1c67202-fb6e-48ff-b497-0ac443f8a823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 50, 2237, 2325, 2259, 3944, 6001, 2051, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"NLP는 정말 재미있어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8eb4598-f752-4b82-bb09-4903ef2938e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] NLP는 정말 재미있어 [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2, 50, 2237, 2325, 2259, 3944, 6001, 2051, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ef79d72-f15f-47ec-b668-f24dd6f7aa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정말 재미있어'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떻게 알아서 떼내어주는거지..\n",
    "tokenizer.decode([3944,6001, 2051])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf9146-ebb5-4a2b-9ca3-4817040ba302",
   "metadata": {},
   "source": [
    "# Tokenizer 궁금한 상황 추가\n",
    "* vocab을 추가할 때 vocab의 갯수(즉 vocab의 길이) 와 차원 추가 여부 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b26a0-9664-4c2a-a509-ffec74599568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 훈련 부분 : https://colab.research.google.com/drive/1TWyT0wTv0Z0hVjLDMaG4qcNZZtdCgGux#scrollTo=VfEKVuu6GnW7\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "813c9b01-0da2-419b-bcd6-1b198b5dd2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before adding\n",
      "Embedding(32000, 768, padding_idx=0)\n",
      "32000\n",
      "after adding\n",
      "Embedding(32000, 768, padding_idx=0)\n",
      "after resizing\n",
      "Embedding(32006, 768)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "# # 모델을 불러오기전에 vocab을 수정하면 pretrained config와 충돌이 일어나 에러가 발생하니 주의\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# before add\n",
    "print('before adding')\n",
    "print(model.get_input_embeddings())\n",
    "print(len(tokenizer))\n",
    "\n",
    "# special token 추가하기 \n",
    "special_tokens_dict = {'additional_special_tokens': ['[special1]','[special2]','[special3]','[special4]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# token 추가하기\n",
    "new_tokens = ['COVID', 'hospitalization']\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# # 기존 config로 모델을 불러오기\n",
    "# # 모델을 불러오기전에 vocab을 수정하면 pretrained config와 충돌이 일어나 에러가 발생하니 주의\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "#     model_name,\n",
    "#     config=config,\n",
    "# )\n",
    "print('after adding')\n",
    "print(model.get_input_embeddings())\n",
    "# tokenizer config 수정해주기 (추후에 발생할 에러를 줄이기 위해)\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "print('after resizing')\n",
    "\n",
    "# model의 token embedding 사이즈 수정하기\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.get_input_embeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f46860-e046-4ff0-8682-1a233699110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
